# python ex_finetune.py hydra.run.dir="/net/vol/werning/.../version0/" checkpoint="epoch\=35-step\=164896.ckpt"
# structure: hydra.run.dir/"checkpoints"/checkpoint
# do not forget model: model.net.width_mult=4.0 (pretrained_name?)
defaults:
  - _self_
  - db: esc50_logits_sd
  - system: nt
  - model: train
hydra:
  run:
    dir: ${system.storage_root}/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
loader:
  batch_size: 64
  num_workers: 2
feature_extractor:
  _target_: EfficientAT.models.preprocess.AugmentMelSTFT
  sr: ${db.sample_rate}
  win_length: 800
  hopsize: 320
  n_mels: 128
  freqm: 0
  timem: 0
optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-5
  weight_decay: 0.0001
trainer:
  _target_: pytorch_lightning.Trainer
  accelerator: "gpu"
  devices: 1
  num_nodes: 1
  logger:
    _target_: lightning.pytorch.loggers.TensorBoardLogger
    save_dir: ${hydra:run.dir}
    name: ${experiment_name}
  # strategy: ddp
  precision: 32
  max_epochs: ${epochs}
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
lr_schedule:
  warm_up_len: 0.2
  ramp_down_start: 0.4
  ramp_down_len: 0.4
  last_lr_value: 0.01
epochs: 50
experiment_name: target_distillation_ft
checkpoint: null
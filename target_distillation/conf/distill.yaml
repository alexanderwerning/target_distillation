defaults:
  - db: audioset_logits_sd
  - system: nt
hydra:
  run:
    dir: ${system.storage_root}/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
loader:
  batch_size: 30
  num_workers: 8
feature_extractor:
  _target_: EfficientAT.models.preprocess.AugmentMelSTFT
  sr: ${db.sample_rate}
  win_length: 800
  hopsize: 320
  n_mels: 128
model:
  _target_: target_distillation.model.LogitDistillationModel
  student:
    _target_: target_distillation.model_wrapper.ModelWrapper
    model:
      _target_: EfficientAT.models.MobileNetV3.get_model
      pretrained_name: mn10_im
      width_mult: 1.0
      num_classes: ${db.num_classes}
  feature_extractor: ${feature_extractor}
  label_loss_prop: 0.1
optimizer:
  _target_: padertorch.contrib.aw.optimizer.AdamW
  lr: 8e-4
  gradient_clipping: 1.0
  weight_decay: 0.0001
trainer:
  _target_: pytorch_lightning.Trainer
  accelerator: "gpu"
  num_nodes: 1
  logger:
    _target_: lightning.pytorch.loggers.TensorBoardLogger
    save_dir: ${hydra:run.dir}
    name: ${experiment_name}
  strategy: ddp
  # limit_train_batches: ${num_iterations}
  precision: 32
  max_epochs: ${epochs}
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
lr_schedule:
  warm_up_len: 0.04
  ramp_down_start: 0.4
  ramp_down_len: 0.475
  last_lr_value: 0.01
epochs: 100
num_iterations: 167000
experiment_name: target_distillation